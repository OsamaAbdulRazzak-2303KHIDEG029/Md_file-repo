{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Retrieve data from qdrant</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gen_pipeline import GenPipeline\n",
    "from src.utils import make_filter\n",
    "\n",
    "gen_pipeline = GenPipeline()\n",
    "index = gen_pipeline._get_qdrant_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"provide me Investment Portfolio of Alfalah GHP Islamic Dedicated Equity Fund - Compliance Report\"\n",
    "retriever = index.as_retriever(similarity_top_k=40, filters=make_filter(query))\n",
    "retrieve_nodes = retriever.retrieve(query)\n",
    "retrieve_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Format retrieved chunks</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_retrieved_chunks(retrieved_chunks):\n",
    "    formatted_texts = []\n",
    "\n",
    "    for node_with_score in retrieved_chunks:\n",
    "        node = node_with_score.node\n",
    "        metadata = node.metadata\n",
    "\n",
    "        # Extract metadata\n",
    "        year = metadata.get(\"year\", None)\n",
    "        month = metadata.get(\"month\", None)\n",
    "        filename = metadata.get(\"filename\", \"N/A\")\n",
    "        text_metadata = metadata.get(\"text_metadata\", None)\n",
    "        text_content = node.text\n",
    "\n",
    "        # Format text according to the desired output\n",
    "        formatted_text = \"\"\n",
    "\n",
    "        if year:\n",
    "            formatted_text += f\"year: {year}\\n\"\n",
    "        if month:\n",
    "            formatted_text += f\"month: {month}\\n\"\n",
    "\n",
    "        formatted_text += f\"filename: {filename}\\n\"\n",
    "\n",
    "        if text_metadata:\n",
    "            formatted_text += f\"text_content: {text_content}\\n{text_metadata}\\n------------------------------\"\n",
    "        else:\n",
    "            formatted_text += (\n",
    "                f\"text_content: {text_content}\\n--------------------------\"\n",
    "            )\n",
    "        formatted_texts.append(formatted_text)\n",
    "\n",
    "    # Join all the formatted texts together\n",
    "    return \"\\n\".join(formatted_texts)\n",
    "\n",
    "\n",
    "# Format the retrieved chunks and print them\n",
    "formated_nodes = format_retrieved_chunks(retrieve_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formated_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Llama index Token Counter</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.callbacks import CallbackManager, TokenCountingHandler\n",
    "from llama_index.core import Settings\n",
    "import tiktoken\n",
    "\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n",
    ")\n",
    "\n",
    "Settings.callback_manager = CallbackManager([token_counter])\n",
    "print(\"prompt: \", token_counter.llm_token_counts[0].prompt[:100], \"...\\n\")\n",
    "print(\n",
    "    \"prompt token count: \",\n",
    "    token_counter.llm_token_counts[0].prompt_token_count,\n",
    "    \"\\n\",\n",
    ")\n",
    "\n",
    "print(\"completion: \", token_counter.llm_token_counts[0].completion[:100], \"...\\n\")\n",
    "print(\n",
    "    \"completion token count: \",\n",
    "    token_counter.llm_token_counts[0].completion_token_count,\n",
    "    \"\\n\",\n",
    ")\n",
    "\n",
    "print(\"total token count\", token_counter.llm_token_counts[0].total_token_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Embedding Tokens: \",\n",
    "    token_counter.total_embedding_token_count,\n",
    "    \"\\n\",\n",
    "    \"LLM Prompt Tokens: \",\n",
    "    token_counter.prompt_llm_token_count,\n",
    "    \"\\n\",\n",
    "    \"LLM Completion Tokens: \",\n",
    "    token_counter.completion_llm_token_count,\n",
    "    \"\\n\",\n",
    "    \"Total LLM Token Count: \",\n",
    "    token_counter.total_llm_token_count,\n",
    "    \"\\n\",\n",
    "    token_counter.llm_token_counts,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Cost Calculator</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the costs per token in dollars\n",
    "prompt_token_cost = 0.50 / 1_000_000  # $0.50 per 1M tokens\n",
    "completion_token_cost = 1.50 / 1_000_000  # $1.50 per 1M tokens\n",
    "\n",
    "# Define the number of tokens\n",
    "prompt_tokens = 5866\n",
    "completion_tokens = 48\n",
    "\n",
    "# Calculate the cost\n",
    "total_prompt_cost = prompt_tokens * prompt_token_cost\n",
    "total_completion_cost = completion_tokens * completion_token_cost\n",
    "total_cost = total_prompt_cost + total_completion_cost\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total cost: ${total_cost:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Reset token counter</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter.reset_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LLM Rerank</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import LLMRerank\n",
    "from pprint import pprint\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "# llm = ChatOpenAI(\n",
    "#     model=self.model_name,\n",
    "#     temperature=0.0,\n",
    "#     verbose=True,\n",
    "#     streaming=True,\n",
    "#     stream_usage=True,\n",
    "# )\n",
    "\n",
    "postprocessor = LLMRerank(choice_batch_size=20, top_n=5, llm=llm)\n",
    "pprint(postprocessor)\n",
    "# postprocessor = LLMRerank(choice_batch_size=10, top_n=5)\n",
    "rerank_retrieve_nodes = postprocessor.postprocess_nodes(retrieve_nodes, query_str=query)\n",
    "print(type(rerank_retrieve_nodes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> LLMRank error handling functions</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging as logger\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "\n",
    "\n",
    "def rerank_retrieve_nodes(retrieve_nodes, query_str, reranker):\n",
    "    attempts = 0\n",
    "    retries = 3\n",
    "    while attempts < retries:\n",
    "        try:\n",
    "            # Perform reranking\n",
    "            reranked_nodes = reranker.postprocess_nodes(\n",
    "                retrieve_nodes, query_str=query_str\n",
    "            )\n",
    "            print(f\"reranked_nodes = {reranked_nodes}\")\n",
    "            logger.info(f\"reranked_nodes = {reranked_nodes}\")\n",
    "            return reranked_nodes  # Exit loop on success\n",
    "        except (ValueError, IndexError) as e:\n",
    "            # Log specific error details\n",
    "            attempts += 1\n",
    "            logger.error(\n",
    "                f\"Error during reranking on attempt {attempts}/{retries}: {type(e).__name__} - {e}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # Catch any other unexpected errors\n",
    "            attempts += 1\n",
    "            logger.error(\n",
    "                f\"Unexpected error during reranking on attempt {attempts}/{retries}: {type(e).__name__} - {e}\"\n",
    "            )\n",
    "    logger.error(f\"Reranking failed after all retries. on query: '{query_str}'\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranker = LLMRerank(choice_batch_size=10, top_n=10)\n",
    "rerank_retrieve_nodes(retrieve_nodes, query, reranker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def rerank_retrieve_nodes(retrieve_nodes, query_str, llm):\n",
    "    attempts = 0\n",
    "    retries = 3\n",
    "    while attempts < retries:\n",
    "        try:\n",
    "            # Initialize LLMRerank postprocessor\n",
    "            postprocessor = LLMRerank(choice_batch_size=10, top_n=5, llm=llm)\n",
    "\n",
    "            # Perform reranking\n",
    "            reranked_nodes = postprocessor.postprocess_nodes(\n",
    "                retrieve_nodes, query_str=query_str\n",
    "            )\n",
    "            print(\"--------------------------------\")\n",
    "            # print(reranked_nodes)\n",
    "            return reranked_nodes  # Exit loop on success\n",
    "        except (ValueError, IndexError) as e:\n",
    "            # Log specific error details\n",
    "            attempts += 1\n",
    "            logging.error(\n",
    "                f\"Error during reranking on attempt {attempts}/{retries}: {type(e).__name__} - {e}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            # Catch any other unexpected errors\n",
    "            attempts += 1\n",
    "            logging.error(\n",
    "                f\"Unexpected error during reranking on attempt {attempts}/{retries}: {type(e).__name__} - {e}\"\n",
    "            )\n",
    "    logging.error(\"Reranking failed after all retries. Returning None.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_retrieve_nodes(retrieve_nodes, query, llm, retries=3, delay=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "# We choose a model with relatively high speed and decent accuracy.\n",
    "postprocessor = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", top_n=5\n",
    ")\n",
    "\n",
    "retrieve_nodes = postprocessor.postprocess_nodes(retrieve_nodes, query_str=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> SimilarityPostprocessor Reranker </h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "postprocessor = SimilarityPostprocessor(similarity_cutoff=0.7)\n",
    "\n",
    "postprocessor.postprocess_nodes(retrieve_nodes, query_str=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Long context reorder</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import LongContextReorder\n",
    "\n",
    "postprocessor = LongContextReorder()\n",
    "\n",
    "postprocessor.postprocess_nodes(retrieve_nodes, query_str=query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Create nodes data and nodes from input dir(handle best performing funds too)</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.readers.file.flat import FlatReader\n",
    "from llama_index.core.schema import TextNode, RelatedNodeInfo, NodeRelationship\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.core.vector_stores import (\n",
    "    MetadataFilter,\n",
    "    MetadataFilters,\n",
    "    FilterOperator,\n",
    "    FilterCondition,\n",
    ")\n",
    "from deep_translator import GoogleTranslator\n",
    "from src.config import MONTH_FULL_NAMES, MONTH_PATTERN, YEAR_PATTERN\n",
    "from src.utils import extract_month, extract_year\n",
    "\n",
    "\n",
    "def creat_node_data_from_input_dir(inpur_dir):\n",
    "\n",
    "    documents = SimpleDirectoryReader(\n",
    "        input_dir=inpur_dir,\n",
    "        file_extractor={\n",
    "            \".md\": FlatReader()\n",
    "        },  # This disables the MarkdownReader for .md files\n",
    "        recursive=True,\n",
    "    ).load_data()\n",
    "\n",
    "    nodes_data = []\n",
    "\n",
    "    for document in documents:\n",
    "        markdown_document = document.get_content()\n",
    "        filename = document.metadata.get(\"filename\")\n",
    "        file_id = document.id_\n",
    "        headers_to_split_on = [\n",
    "            (\"#\", \"Header 1\"),\n",
    "            # (\"##\", \"Header 2\"),\n",
    "            # (\"###\", \"Header 3\"),\n",
    "        ]\n",
    "        markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "            headers_to_split_on=headers_to_split_on\n",
    "        )\n",
    "        md_header_splits = markdown_splitter.split_text(markdown_document)\n",
    "\n",
    "        node_data = {\"file_id\": file_id, \"filename\": filename, \"node_text\": []}\n",
    "\n",
    "        for text in md_header_splits:\n",
    "            headers_combined = []\n",
    "\n",
    "            # Loop through metadata and concatenate headers\n",
    "            for _, header in text.metadata.items():\n",
    "                if header:\n",
    "                    headers_combined.append(header)\n",
    "\n",
    "            headers_combined = \" of \".join(headers_combined[::-1])\n",
    "            # Concatenate headers and page content\n",
    "            concat_text = headers_combined + \"\\n\" + text.page_content\n",
    "            node_data[\"node_text\"].append(concat_text)\n",
    "        nodes_data.append(node_data)\n",
    "\n",
    "    return nodes_data\n",
    "\n",
    "\n",
    "# pattern = r\"(Top Performing Funds and Returns for .+)\\n(title\\|subtitle\\|percent\\|timeperiod\\n([\\w\\s\\-\\–().|%–:]+))\"\n",
    "# pattern = r\"(# .+?)\\n+(\\| title\\s+\\| subtitle\\s+\\| percent\\s+\\| timeperiod\\s+\\|[\\s\\S]+?)(?=\\n\\s*#|\\Z)\"\n",
    "# pattern = r\"^(Top Performing Funds and Returns for .+?)\\n(\\|.+?)(?=\\n[A-Z#]|$)\"\n",
    "\n",
    "\n",
    "# def create_nodes_from_nodes_data(nodes_data):\n",
    "#     nodes = []\n",
    "#     for data in nodes_data:\n",
    "#         filename = data[\"filename\"]\n",
    "#         month = extract_month(filename)\n",
    "#         year = extract_year(filename)\n",
    "#         if month and year:\n",
    "#             node_text = data[\"node_text\"][0]\n",
    "#             match = re.search(pattern, node_text, re.DOTALL)\n",
    "#             if match and len(data[\"node_text\"]) == 1:\n",
    "#                 print(f'lenght of nodes {len(data[\"node_text\"])}')\n",
    "#                 report_title = match.group(1)\n",
    "#                 report_content = match.group(2)\n",
    "#                 node = TextNode(\n",
    "#                     text=report_title,\n",
    "#                     metadata={\n",
    "#                         \"year\": str(year),\n",
    "#                         \"month\": month,\n",
    "#                         \"filename\": filename,\n",
    "#                         \"file_id\": data[\"file_id\"],\n",
    "#                         \"text_metadata\": report_content,\n",
    "#                     },\n",
    "#                 )\n",
    "#                 node.excluded_embed_metadata_keys = [\"text_metadata\", \"file_id\"]\n",
    "#                 print(\"iam here\")\n",
    "#                 node.relationships[NodeRelationship.SOURCE] = RelatedNodeInfo(\n",
    "#                     node_id=data[\"file_id\"], metadata={\"filename\": filename}\n",
    "#                 )\n",
    "#                 nodes.append(node)\n",
    "#             else:\n",
    "#                 for text in data[\"node_text\"]:\n",
    "#                     node = TextNode(\n",
    "#                         text=(text),\n",
    "#                         metadata={\n",
    "#                             \"year\": str(year),\n",
    "#                             \"month\": month,\n",
    "#                             \"filename\": filename,\n",
    "#                             \"file_id\": data[\"file_id\"],\n",
    "#                         },\n",
    "#                     )\n",
    "#                     node.excluded_embed_metadata_keys = [\"file_id\"]\n",
    "#                     node.relationships[NodeRelationship.SOURCE] = RelatedNodeInfo(\n",
    "#                         node_id=data[\"file_id\"], metadata={\"filename\": filename}\n",
    "#                     )\n",
    "#                     nodes.append(node)\n",
    "#         else:\n",
    "#             for text in data[\"node_text\"]:\n",
    "#                 node = TextNode(\n",
    "#                     text=(text),\n",
    "#                     metadata={\n",
    "#                         \"filename\": filename,\n",
    "#                         \"file_id\": data[\"file_id\"],\n",
    "#                     },\n",
    "#                 )\n",
    "#                 node.excluded_embed_metadata_keys = [\"file_id\"]\n",
    "#                 node.relationships[NodeRelationship.SOURCE] = RelatedNodeInfo(\n",
    "#                     node_id=data[\"file_id\"], metadata={\"filename\": filename}\n",
    "#                 )\n",
    "\n",
    "#                 nodes.append(node)\n",
    "\n",
    "#     return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define patterns for both types of reports\n",
    "pattern_top_performing_with_month_and_year = (\n",
    "    r\"^(Top/Best Performing Funds and Returns for .+?)\\n(\\|.+?)(?=\\n[A-Z#]|$)\"\n",
    ")\n",
    "pattern_top_performing = (\n",
    "    r\"^(Top/Best Performing Funds and Returns)\\n(\\|.+?)(?=\\n[A-Z#]|$)\"\n",
    ")\n",
    "pattern_fund_names = r\"(#?\\s*Name of all Funds offered by AAML.*?Profile)\\n([\\s\\S]*)\"\n",
    "\n",
    "\n",
    "def create_nodes_from_nodes_data(nodes_data):\n",
    "    nodes = []\n",
    "    for data in nodes_data:\n",
    "        filename = data[\"filename\"]\n",
    "        month = extract_month(filename)\n",
    "        year = extract_year(filename)\n",
    "\n",
    "        node_text = data[\"node_text\"][0]\n",
    "\n",
    "        # Try matching \"Top Performing Funds with month and year\"\n",
    "        match_top_performing__with_month_and_year = re.search(\n",
    "            pattern_top_performing_with_month_and_year, node_text, re.DOTALL\n",
    "        )\n",
    "\n",
    "        # Try matching \"Top Performing Funds\"\n",
    "        match_top_performing = re.search(pattern_top_performing, node_text, re.DOTALL)\n",
    "\n",
    "        print(match_top_performing__with_month_and_year)\n",
    "        print(\"---------------------------------------\")\n",
    "\n",
    "        # Try matching \"Name of all Funds offered by AAML\"\n",
    "        match_fund_names = re.search(pattern_fund_names, node_text, re.DOTALL)\n",
    "\n",
    "        if month and year:\n",
    "            if match_top_performing__with_month_and_year:\n",
    "                report_title = match_top_performing__with_month_and_year.group(1)\n",
    "                report_content = match_top_performing__with_month_and_year.group(2)\n",
    "            elif match_fund_names:\n",
    "                report_title = match_fund_names.group(1)\n",
    "                report_content = match_fund_names.group(2)\n",
    "            else:\n",
    "                report_title = None\n",
    "                report_content = None\n",
    "\n",
    "            if report_title and report_content and len(data[\"node_text\"]) == 1:\n",
    "                node = TextNode(\n",
    "                    text=report_title,\n",
    "                    metadata={\n",
    "                        \"year\": str(year),\n",
    "                        \"month\": month,\n",
    "                        \"filename\": filename,\n",
    "                        \"file_id\": data[\"file_id\"],\n",
    "                        \"text_metadata\": report_content,\n",
    "                    },\n",
    "                )\n",
    "                node.excluded_embed_metadata_keys = [\"text_metadata\", \"file_id\"]\n",
    "                node.relationships[NodeRelationship.SOURCE] = RelatedNodeInfo(\n",
    "                    node_id=data[\"file_id\"], metadata={\"filename\": filename}\n",
    "                )\n",
    "                nodes.append(node)\n",
    "\n",
    "            else:\n",
    "                for text in data[\"node_text\"]:\n",
    "                    node = TextNode(\n",
    "                        text=text,\n",
    "                        metadata={\n",
    "                            \"year\": str(year),\n",
    "                            \"month\": month,\n",
    "                            \"filename\": filename,\n",
    "                            \"file_id\": data[\"file_id\"],\n",
    "                        },\n",
    "                    )\n",
    "                    node.excluded_embed_metadata_keys = [\"file_id\"]\n",
    "                    node.relationships[NodeRelationship.SOURCE] = RelatedNodeInfo(\n",
    "                        node_id=data[\"file_id\"], metadata={\"filename\": filename}\n",
    "                    )\n",
    "                    nodes.append(node)\n",
    "        else:\n",
    "            print(\"bro i am here eeee\")\n",
    "\n",
    "            if match_top_performing:\n",
    "                report_title = match_top_performing.group(1)\n",
    "                report_content = match_top_performing.group(2)\n",
    "\n",
    "            elif match_fund_names:\n",
    "                print(\"✅ Match Found for Fund Names\")\n",
    "                # print(\"Title:\", match_fund_names.group(1))\n",
    "                # print(\"Content:\", match_fund_names.group(2))\n",
    "                report_title = match_fund_names.group(1)\n",
    "                print(\"text:\", report_title)\n",
    "                report_content = match_fund_names.group(2)\n",
    "            else:\n",
    "                report_title = None\n",
    "                report_content = None\n",
    "            if report_title and report_content and len(data[\"node_text\"]) == 1:\n",
    "                node = TextNode(\n",
    "                    text=report_title,\n",
    "                    metadata={\n",
    "                        \"filename\": filename,\n",
    "                        \"file_id\": data[\"file_id\"],\n",
    "                        \"text_metadata\": report_content,\n",
    "                    },\n",
    "                )\n",
    "                node.excluded_embed_metadata_keys = [\"text_metadata\", \"file_id\"]\n",
    "                node.relationships[NodeRelationship.SOURCE] = RelatedNodeInfo(\n",
    "                    node_id=data[\"file_id\"], metadata={\"filename\": filename}\n",
    "                )\n",
    "                nodes.append(node)\n",
    "            else:\n",
    "                for text in data[\"node_text\"]:\n",
    "                    node = TextNode(\n",
    "                        text=text,\n",
    "                        metadata={\n",
    "                            \"filename\": filename,\n",
    "                            \"file_id\": data[\"file_id\"],\n",
    "                        },\n",
    "                    )\n",
    "                    node.excluded_embed_metadata_keys = [\"file_id\"]\n",
    "                    node.relationships[NodeRelationship.SOURCE] = RelatedNodeInfo(\n",
    "                        node_id=data[\"file_id\"], metadata={\"filename\": filename}\n",
    "                    )\n",
    "                    nodes.append(node)\n",
    "\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "path = \"latest_modified_fmr_data/Alfalah_assist_all_tabulor_md_data\"\n",
    "nodes_data = creat_node_data_from_input_dir(path)\n",
    "nodes = create_nodes_from_nodes_data(nodes_data)\n",
    "print(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the directory containing your .md files\n",
    "input_directory = (\n",
    "    \"latest_modified_fmr_data/alfalah assist table data/Alfalah_assist_tabulor_md_data\"\n",
    ")\n",
    "output_file = \"Alfalah_assist_all_tabulor_md_data.md\"\n",
    "\n",
    "# Get all .md files sorted by name\n",
    "md_files = sorted([f for f in os.listdir(input_directory) if f.endswith(\".md\")])\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for filename in md_files:\n",
    "        filepath = os.path.join(input_directory, filename)\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as infile:\n",
    "            outfile.write(f\"\\n\\n# {filename}\\n\\n\")  # Optional: file header\n",
    "            outfile.write(infile.read())\n",
    "            outfile.write(\"\\n\")  # Ensure separation between files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(str(nodes[0]), width=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Delete data from Qdrant</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# from src.utils import creat_node_data_from_input_dir, create_nodes_from_nodes_data\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient, models\n",
    "import qdrant_client\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_data_from_qdrant(filename: str):\n",
    "    # Initialize Qdrant client\n",
    "    client = qdrant_client.QdrantClient(url=\"http://65.0.229.53:6333\", port=6333)\n",
    "\n",
    "    COLLECTION_NAME = \"alfalah_investment\"\n",
    "    embed_model_name = \"text-embedding-3-small\"\n",
    "\n",
    "    # Set the embedding model\n",
    "    embed_model = OpenAIEmbedding(model_name=embed_model_name)\n",
    "    Settings.embed_model = embed_model\n",
    "\n",
    "    # Perform the delete operation\n",
    "    try:\n",
    "        response = client.delete(\n",
    "            collection_name=COLLECTION_NAME,  # Use the variable directly\n",
    "            points_selector=models.FilterSelector(\n",
    "                filter=models.Filter(\n",
    "                    must=[\n",
    "                        models.FieldCondition(\n",
    "                            key=\"filename\",\n",
    "                            match=models.MatchValue(value=filename),\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "            ),\n",
    "        )\n",
    "        print(f\"File for the year '{filename}' has been deleted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_data_from_qdrant(\"QUERIES_FOR_CHATBOT.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Add data to Qdrant</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def add_data_to_qdrant(path):\n",
    "\n",
    "    client = qdrant_client.QdrantClient(url=\"http://65.0.229.53:6333\", port=6333)\n",
    "\n",
    "    COLLECTION_NAME = \"alfalah_investment\"\n",
    "    embed_model_name = \"text-embedding-3-small\"\n",
    "\n",
    "    embed_model = OpenAIEmbedding(model_name=embed_model_name)\n",
    "    Settings.embed_model = embed_model\n",
    "\n",
    "    vector_store = QdrantVectorStore(client=client, collection_name=COLLECTION_NAME)\n",
    "    nodes_data = creat_node_data_from_input_dir(path)\n",
    "    nodes = create_nodes_from_nodes_data(nodes_data)\n",
    "\n",
    "    print(nodes)\n",
    "\n",
    "    logging.info(\"no collection found\")\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        vector_store=vector_store,\n",
    "    )\n",
    "    index = VectorStoreIndex(nodes, storage_context=storage_context)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_data_to_qdrant(\"latest_modified_fmr_data/FAQs/queries for chatbot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Prcesssing md Files</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def replace_in_files(folder_path: str):\n",
    "    \"\"\"\n",
    "    Reads all files in the given folder, replaces specific headers with their updated versions,\n",
    "    and saves the changes back to the files.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder containing the files.\n",
    "    \"\"\"\n",
    "    # Mapping of original headers to their replacements\n",
    "    replacements = {\n",
    "        \"\\nSindh Workers\": \"\\n## Sindh Workers\",\n",
    "        \"\\n(Holdings as % of Total Assets)\": \"\\n### (Holdings as % of Total Assets)\",\n",
    "        \"\\nHoldings as % of Total Assets\": \"\\n### (Holdings as % of Total Assets)\",\n",
    "        \"\\nFund Statistics:\": \"\\n### Fund Statistics:\",\n",
    "        \"\\nFund Statistic\": \"\\n### Fund Statistic:\",\n",
    "        \"\\nFund Statistics\": \"\\n### Fund Statistics:\",\n",
    "        \"\\nFund Stataistics\": \"\\n### Fund Stataistics:\",\n",
    "        \"\\nfund statistics\": \"\\n### fund statistics:\",\n",
    "        \"\\nTop Ten Holdings (as a % of total assets)\": \"\\n### Top Ten Holdings (as a % of total assets)\",\n",
    "        \"\\nSector Allocation (as a % of total assets)\": \"\\n### Sector Allocation (as a % of total assets)\",\n",
    "        \"\\nAsset Allocation (as % of Total Assets)\": \"\\n### Asset Allocation (as % of Total Assets)\",\n",
    "        \"\\nRisk Profile:\": \"\\n## Risk Profile:\",\n",
    "        \"\\nRisk Profile\": \"\\n## Risk Profile:\",\n",
    "        \"\\nFund Performance\": \"\\n### Fund Performance\",\n",
    "        \"\\nFund Performanace\": \"\\n### Fund Performanace\",\n",
    "        \"\\nFund Performanace:\": \"\\n### Fund Performanace:\",\n",
    "        \"\\nFund Perfomance\": \"\\n### Fund Perfomance\",\n",
    "        \"\\nfund performance\": \"\\n### Fund Performance\",\n",
    "        \"\\nPerformance\": \"\\n### Performance\",\n",
    "        \"\\nAsset Allocation\": \"\\n### Asset Allocation\",\n",
    "        \"\\n### RISK PROFILE OF ISLAMIC COLLECTIVE INVESTMENT SCHEMES/PLANS\": \"\\n# RISK PROFILE OF ISLAMIC COLLECTIVE INVESTMENT SCHEMES/PLANS\",\n",
    "        \"\\n### RISK PROFILE OF CONVENTIONAL COLLECTIVE INVESTMENT SCHEMES/PLANS\": \"\\n# RISK PROFILE OF CONVENTIONAL COLLECTIVE INVESTMENT SCHEMES/PLANS\",\n",
    "    }\n",
    "\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                # Read the file\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "\n",
    "                # Apply all replacements\n",
    "                for original, replacement in replacements.items():\n",
    "                    content = content.replace(original, replacement)\n",
    "\n",
    "                # Write back the updated content\n",
    "                with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(content)\n",
    "\n",
    "                print(f\"Processed file: {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"latest_modified_fmr_data/single_modified_file\"\n",
    "replace_in_files(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Create Filters</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import (\n",
    "    MetadataFilter,\n",
    "    MetadataFilters,\n",
    "    FilterOperator,\n",
    "    FilterCondition,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filters_for_all_data(month, year):\n",
    "    filters_list = []\n",
    "\n",
    "    # If a month is found, add a month filter\n",
    "    if month:\n",
    "        filters_list.append(\n",
    "            MetadataFilter(key=\"month\", operator=FilterOperator.EQ, value=month)\n",
    "        )\n",
    "\n",
    "    # If a year is found, add a year filter\n",
    "    if year:\n",
    "        filters_list.append(\n",
    "            MetadataFilter(key=\"year\", operator=FilterOperator.EQ, value=year)\n",
    "        )\n",
    "\n",
    "    # Return filters if any are found, otherwise None\n",
    "    print(filters_list)\n",
    "    if filters_list:\n",
    "        return MetadataFilters(filters=filters_list, condition=FilterCondition.AND)\n",
    "    return MetadataFilters(\n",
    "        filters=[\n",
    "            # MetadataFilter(key=\"year\", operator=FilterOperator.NIN, value=list(range(2014, 2024))),\n",
    "            MetadataFilter(\n",
    "                key=\"year\", operator=FilterOperator.IS_EMPTY, value=None\n",
    "            )  # Avoid including `value`\n",
    "        ],\n",
    "        condition=FilterCondition.OR,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_filters_for_all_data(\"nov\", \"2024\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filters_for_specific_files(file1, file2):\n",
    "    filters_list = []\n",
    "\n",
    "    # Add a filter for the first file\n",
    "    if file1:\n",
    "        filters_list.append(\n",
    "            MetadataFilter(key=\"filename\", operator=FilterOperator.EQ, value=file1)\n",
    "        )\n",
    "\n",
    "    # Add a filter for the second file\n",
    "    if file2:\n",
    "        filters_list.append(\n",
    "            MetadataFilter(key=\"filename\", operator=FilterOperator.EQ, value=file2)\n",
    "        )\n",
    "\n",
    "    # Combine the filters with an OR condition to get results from both files\n",
    "    return MetadataFilters(filters=filters_list, condition=FilterCondition.OR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_filter_for_specific_file(query):\n",
    "    # Extract month and year from the query\n",
    "    file1 = \"Conventional_all_Fund_Data.md\"\n",
    "    file2 = \"Islamic_all_Fund_Data.md\"\n",
    "    # Create and return filters based on extracted month and year\n",
    "    return create_filters_for_specific_files(file1, file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.gen_pipeline import GenPipeline\n",
    "from src.utils import make_filter\n",
    "\n",
    "gen_pipeline = GenPipeline()\n",
    "index = gen_pipeline._get_qdrant_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"tell me fund performance for all funds \"\n",
    "retriever = index.as_retriever(\n",
    "    similarity_top_k=20, filters=make_filter_for_specific_file(query)\n",
    ")\n",
    "retrieve_nodes = retriever.retrieve(query)\n",
    "retrieve_nodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
